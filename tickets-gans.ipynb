{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6876620,"sourceType":"datasetVersion","datasetId":3951380}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow.keras as tfk\nimport tensorflow.keras.layers as tfkl\nimport pandas as pd\nimport os\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:39:42.606916Z","iopub.execute_input":"2023-11-15T14:39:42.607909Z","iopub.status.idle":"2023-11-15T14:39:46.114149Z","shell.execute_reply.started":"2023-11-15T14:39:42.607797Z","shell.execute_reply":"2023-11-15T14:39:46.113251Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/ah-tickets/tickets.csv')","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:39:46.115809Z","iopub.execute_input":"2023-11-15T14:39:46.116412Z","iopub.status.idle":"2023-11-15T14:39:46.933474Z","shell.execute_reply.started":"2023-11-15T14:39:46.116380Z","shell.execute_reply":"2023-11-15T14:39:46.932428Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:39:46.934747Z","iopub.execute_input":"2023-11-15T14:39:46.935165Z","iopub.status.idle":"2023-11-15T14:39:46.956416Z","shell.execute_reply.started":"2023-11-15T14:39:46.935129Z","shell.execute_reply":"2023-11-15T14:39:46.955529Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Helper():\n  def __init__(self,data):\n    self.data = data\n  def plot_data(self,save_path):\n    num_rows = 2\n    num_cols = 9\n    fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(30, 8))\n    \n    features = self.data.columns  # Assuming features are column names\n    for i, feature in enumerate(features):\n        row_index = i // num_cols\n        col_index = i % num_cols\n\n        unique_values = self.data[feature].unique()\n        value_counts = self.data[feature].value_counts()\n\n        axes[row_index, col_index].bar(unique_values, value_counts)\n        axes[row_index, col_index].set_title(feature)\n    if save_path:\n        plt.savefig(save_path)\n\n    plt.show()\n    return features\n  def drop_date(self):\n    columns_to_drop = ['creation_date', 'view_date','action_date']\n    self.data = self.data.drop(columns=columns_to_drop)\n  def replace_values_to_number(self,column_name):\n    list_of_values = self.data[column_name].unique()\n    d = dict(zip(list_of_values, range(len(list_of_values))))\n    for i in range(len(list_of_values)):\n        self.data[column_name] = self.data[column_name].replace(list_of_values[i], i)\n    return d\n  def replace_values_to_number_with_dict(self,column_name, dic):\n      list_of_values = self.data[column_name].unique()\n      for i in range(len(list_of_values)):\n          self.data[column_name] = self.data[column_name].replace(list_of_values[i], list(dic.keys())[i])\n  def replace_dates(self):\n    dates = ['creation','view','action']\n    for date in dates:\n        self.data[f'{date}_date'] = pd.to_datetime(self.data[f'{date}_date'])\n        self.data[f'{date}_month'] = self.data[f'{date}_date'].dt.month\n        self.data[f'{date}_day'] = self.data[f'{date}_date'].dt.day\n        self.data[f'{date}_hour'] = self.data[f'{date}_date'].dt.hour\n        self.data[f'{date}_minute'] = self.data[f'{date}_date'].dt.minute\n  def convert_all_data_sig(self):\n    features = self.data.head(0)\n    thre = []\n    for feature in features:\n      thre.append(1/self.data[feature].max())\n      self.data[feature]= self.data[feature]/self.data[feature].max()\n    return thre\n  def on_hot_encode(self):\n    features = self.data.head(0)\n    encoded_features=[]\n    for feature in features:\n      unique_values = data[feature].unique()\n      encoded = np.eye(len(unique_values))[np.where(data[feature].values[:, None] == unique_values)[1]]\n      encoded_features.append(encoded)\n    return encoded_features","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:39:46.959035Z","iopub.execute_input":"2023-11-15T14:39:46.959550Z","iopub.status.idle":"2023-11-15T14:39:46.975219Z","shell.execute_reply.started":"2023-11-15T14:39:46.959516Z","shell.execute_reply":"2023-11-15T14:39:46.974359Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"H = Helper(data)\nH.replace_dates()\nH.drop_date()\ntit = H.data.head(0)\ndics = []\nfor col in tit:\n    dics.append(H.replace_values_to_number(col))\ncols = H.plot_data('/kaggle/working/real_data.png')\nth_out = H.convert_all_data_sig()\ndata = H.data","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:39:46.976367Z","iopub.execute_input":"2023-11-15T14:39:46.976652Z","iopub.status.idle":"2023-11-15T14:39:58.923107Z","shell.execute_reply.started":"2023-11-15T14:39:46.976630Z","shell.execute_reply":"2023-11-15T14:39:58.921893Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_data = data","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:39:58.924787Z","iopub.execute_input":"2023-11-15T14:39:58.925716Z","iopub.status.idle":"2023-11-15T14:39:58.929929Z","shell.execute_reply.started":"2023-11-15T14:39:58.925673Z","shell.execute_reply":"2023-11-15T14:39:58.929046Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = tf.Variable(data)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:39:58.931822Z","iopub.execute_input":"2023-11-15T14:39:58.932114Z","iopub.status.idle":"2023-11-15T14:40:00.476948Z","shell.execute_reply.started":"2023-11-15T14:39:58.932090Z","shell.execute_reply":"2023-11-15T14:40:00.476095Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GANS_Sigmoid():\n  def __init__(self,data,noise_size,num_epochs,batch_size):\n    self.data = data\n    # Generate some random tabular data for demonstration purposes\n    # feature = 1\n    self.num_samples = self.data.shape[0]\n    self.num_features = self.data.shape[1]\n    self.noise_size = noise_size\n    self.gen = self.generator()\n    self.disc = self.discriminator()\n    self.num_epochs = num_epochs\n    self.batch_size = batch_size\n# real_data = np.random.rand(num_samples, num_features)\n  def Fit(self):\n    disc_losses=[]\n    gen_losses=[]\n    generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n    discriminator_optimizer = tf.keras.optimizers.Adam(1e-3)\n    gen_vars = self.gen .trainable_variables\n    disc_vars = self.disc.trainable_variables\n    generator_optimizer.build(gen_vars)\n    discriminator_optimizer.build(disc_vars)\n    # Training loop\n    num_epochs = self.num_epochs\n    batch_size = self.batch_size\n    for epoch in range(num_epochs):\n        for _ in range(self.num_samples // batch_size):\n            # Train the discriminator\n            with tf.GradientTape() as tape:\n                noise = tf.random.normal((batch_size, self.noise_size))\n                fake_data = self.gen(noise)\n                real_output = self.disc(data[_*batch_size:(_+1)*batch_size])\n                fake_output = self.disc(fake_data)\n                loss = self.discriminator_loss(real_output, fake_output)\n\n\n                # # Build the optimizer with the model's variables\n\n            gradients = tape.gradient(loss, self.disc.trainable_variables)\n            # discriminator_optimizer.apply_gradients(zip(gradients, disc.trainable_variables))\n            discriminator_optimizer.apply_gradients(zip(gradients, disc_vars))\n\n\n            # Train the generator\n            with tf.GradientTape() as tape:\n                fake_data = self.gen(tf.random.normal((batch_size, self.noise_size)))\n                fake_output = self.disc(fake_data)\n                loss = self.generator_loss(fake_output)\n            gradients = tape.gradient(loss, self.gen.trainable_variables)\n            generator_optimizer.apply_gradients(zip(gradients, self.gen.trainable_variables))\n\n        # if epoch % 100 == 0:\n        disc_losses.append(np.mean(self.discriminator_loss(real_output, fake_output)))\n        gen_losses.append(np.mean(self.generator_loss(fake_output)))\n        print(f\"Epoch {epoch}: Discriminator Loss = {np.mean(self.discriminator_loss(real_output, fake_output))}, Generator Loss = {np.mean(self.generator_loss(fake_output))}\")\n    return disc_losses,gen_losses\n# Define the generator and discriminator networks\n  def generator(self):\n      model = tf.keras.Sequential([\n          tf.keras.layers.Dense(128, activation='relu', input_shape=(self.noise_size,)),\n          tf.keras.layers.Dense(256, activation='relu'),\n          tf.keras.layers.Dense(512, activation='relu'),\n          tf.keras.layers.Dense(1024, activation='relu'),\n#            tf.keras.layers.Dense(2024, activation='relu'),\n#            tf.keras.layers.Dense(4048, activation='relu'),\n#            tf.keras.layers.Dense(2024, activation='relu'),\n#            tf.keras.layers.Dense(1024, activation='relu'),\n          tf.keras.layers.Dense(512, activation='relu'),\n          tf.keras.layers.Dense(256, activation='relu'),\n          tf.keras.layers.Dense(128, activation='relu'),\n          tf.keras.layers.Dense(self.num_features,activation='sigmoid')\n      ])\n      return model\n\n  def discriminator(self):\n      model = tf.keras.Sequential([\n          tf.keras.layers.Dense(64, activation='relu', input_shape=(self.num_features,)),\n          tf.keras.layers.Dense(256, activation='relu'),\n          tf.keras.layers.Dense(512, activation='relu'),\n          # tf.keras.layers.Dense(1024, activation='relu'),\n          # tf.keras.layers.Dense(512, activation='relu'),\n          tf.keras.layers.Dense(256, activation='relu'),\n          # tf.keras.layers.Dense(128, activation='relu'),\n          tf.keras.layers.Dense(1, activation='sigmoid')\n      ])\n      return model\n\n# Define the loss function for the discriminator\n  def discriminator_loss(self,real_output, fake_output):\n      real_loss = tf.losses.binary_crossentropy(tf.ones_like(real_output), real_output)\n      fake_loss = tf.losses.binary_crossentropy(tf.zeros_like(fake_output), fake_output)\n      return real_loss + fake_loss\n\n# Define the loss function for the generator\n  def generator_loss(self,fake_output):\n      return tf.losses.binary_crossentropy(tf.ones_like(fake_output), fake_output)\n  def Predict(self,num_samples):\n    output = self.gen(tf.random.normal((num_samples, self.noise_size)))\n    final = np.zeros((num_samples,self.num_features))\n    for i in range(self.num_features):\n      final[:,i] = output[:,i]/th_out[i]\n    final = np.round(final)\n    return final\n  def plot_dist(self, num_samples, feature_names, save_path=None):\n    final = self.Predict(num_samples)\n    \n    # Calculate the number of rows and columns for the subplots\n    num_rows = 2\n    num_cols = 9\n    \n    # Create subplots\n    fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(30, 8))\n    \n    # Flatten the axes array for easier indexing\n    axes = axes.flatten()\n    \n    for i in range(min(self.num_features, num_rows * num_cols)):\n        unique_values, value_counts = np.unique(final[:, i], return_counts=True)\n        axes[i].bar(unique_values, value_counts)\n        axes[i].set_title(f'{feature_names[i]}')  # Use feature names as titles for each subplot\n\n    # Adjust layout to prevent clipping of titles\n    plt.tight_layout()\n\n    if save_path:\n        plt.savefig(save_path)\n\n    plt.show()\n\n\n\n\n# Define the optimizers for the generator and discriminator\n\n\n# Generate synthetic data with the trained generator\n# synthetic_data = generator(tf.random.normal((num_samples, num_features)))\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-15T15:20:45.059631Z","iopub.execute_input":"2023-11-15T15:20:45.059998Z","iopub.status.idle":"2023-11-15T15:20:45.085699Z","shell.execute_reply.started":"2023-11-15T15:20:45.059967Z","shell.execute_reply":"2023-11-15T15:20:45.084478Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"G1 = GANS_Sigmoid(data,512,80,2048)\ndisc,gen = G1.Fit()","metadata":{"execution":{"iopub.status.busy":"2023-11-15T14:40:34.688513Z","iopub.execute_input":"2023-11-15T14:40:34.688885Z","iopub.status.idle":"2023-11-15T14:53:12.166546Z","shell.execute_reply.started":"2023-11-15T14:40:34.688854Z","shell.execute_reply":"2023-11-15T14:53:12.165503Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.title('losses')\nplt.plot(range(len(disc)),disc,label='Discriminator Loss',color='blue')\nplt.plot(range(len(gen)),gen,label='Generator Loss',color='orange')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-15T15:09:38.410605Z","iopub.execute_input":"2023-11-15T15:09:38.411588Z","iopub.status.idle":"2023-11-15T15:09:38.655415Z","shell.execute_reply.started":"2023-11-15T15:09:38.411549Z","shell.execute_reply":"2023-11-15T15:09:38.654477Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"G1.plot_dist(306419,cols,'/kaggle/working/syn_data.png')","metadata":{"execution":{"iopub.status.busy":"2023-11-15T15:20:52.899642Z","iopub.execute_input":"2023-11-15T15:20:52.900113Z","iopub.status.idle":"2023-11-15T15:20:56.740791Z","shell.execute_reply.started":"2023-11-15T15:20:52.900073Z","shell.execute_reply":"2023-11-15T15:20:56.739455Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Syn_sigmoid = G1.Predict(306419)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T15:21:04.799945Z","iopub.execute_input":"2023-11-15T15:21:04.800841Z","iopub.status.idle":"2023-11-15T15:21:05.052897Z","shell.execute_reply.started":"2023-11-15T15:21:04.800806Z","shell.execute_reply":"2023-11-15T15:21:05.052048Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_syn = pd.DataFrame(data=Syn_sigmoid, columns=cols)\n\n# Print the resulting DataFrame\ndf_syn","metadata":{"execution":{"iopub.status.busy":"2023-11-15T15:21:06.461529Z","iopub.execute_input":"2023-11-15T15:21:06.461900Z","iopub.status.idle":"2023-11-15T15:21:06.500763Z","shell.execute_reply.started":"2023-11-15T15:21:06.461870Z","shell.execute_reply":"2023-11-15T15:21:06.499743Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"G1.gen.save('/kaggle/working/model_sig')","metadata":{"execution":{"iopub.status.busy":"2023-11-15T15:14:43.163471Z","iopub.execute_input":"2023-11-15T15:14:43.164373Z","iopub.status.idle":"2023-11-15T15:14:44.407246Z","shell.execute_reply.started":"2023-11-15T15:14:43.164339Z","shell.execute_reply":"2023-11-15T15:14:44.406399Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import entropy\n\ndef compute_kl_divergence(original_data, synthesized_data):\n    kl_divergence = 0.0\n    all_values = set(original_data.values) | set(synthesized_data.values)\n\n    for column in original_data.columns:\n        # Compute probability distributions for each column\n        p = original_data[column].value_counts(normalize=True).reindex(all_values, fill_value=0)\n        q = synthesized_data[column].value_counts(normalize=True).reindex(all_values, fill_value=0)\n\n        # Calculate KL divergence for the current column\n        kl_divergence += entropy(p, q)\n\n    # Aggregate KL divergence across all columns\n    return kl_divergence\n\n# Example usage\nkl_divergence = compute_kl_divergence(df_data, df_syn)\nprint(f\"KL Divergence: {kl_divergence}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-15T15:22:23.349705Z","iopub.execute_input":"2023-11-15T15:22:23.350474Z","iopub.status.idle":"2023-11-15T15:22:23.430101Z","shell.execute_reply.started":"2023-11-15T15:22:23.350438Z","shell.execute_reply":"2023-11-15T15:22:23.428892Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/ah-tickets/tickets.csv')","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"H1 = Helper(data)\nH1.replace_dates()\nH1.drop_date()\ntit = H1.data.head(0)\ndics = []\nfor col in tit:\n    dics.append(H1.replace_values_to_number(col))\nH1.plot_data()\nencoded = H1.on_hot_encode()\n\n# data = H1.data","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GANS_Soft():\n  def __init__(self,data,noise_size,num_epochs,batch_size):\n    self.data = data\n    # Generate some random tabular data for demonstration purposes\n    # feature = 1\n    self.num_samples = self.data.shape[0]\n    self.num_features = self.data.shape[1]\n    self.noise_size = noise_size\n    self.gen = self.generator()\n    self.disc = self.discriminator()\n    self.num_epochs = num_epochs\n    self.batch_size = batch_size\n# real_data = np.random.rand(num_samples, num_features)\n  def Fit(self):\n    disc_losses=[]\n    gen_losses=[]\n    generator_optimizer = tf.keras.optimizers.Adam(1e-2)\n    discriminator_optimizer = tf.keras.optimizers.Adam(1e-3)\n    gen_vars = self.gen .trainable_variables\n    disc_vars = self.disc.trainable_variables\n    generator_optimizer.build(gen_vars)\n    discriminator_optimizer.build(disc_vars)\n    # Training loop\n    num_epochs = self.num_epochs\n    batch_size = self.batch_size\n    for epoch in range(num_epochs):\n        for _ in range(self.num_samples // batch_size):\n            # Train the discriminator\n            with tf.GradientTape() as tape:\n                noise = tf.random.normal((batch_size, self.noise_size))\n                fake_data = self.gen(noise)\n                real_output = self.disc(data[_*batch_size:(_+1)*batch_size])\n                fake_output = self.disc(fake_data)\n                loss = self.discriminator_loss(real_output, fake_output)\n\n\n                # # Build the optimizer with the model's variables\n\n            gradients = tape.gradient(loss, self.disc.trainable_variables)\n            # discriminator_optimizer.apply_gradients(zip(gradients, disc.trainable_variables))\n            discriminator_optimizer.apply_gradients(zip(gradients, disc_vars))\n\n\n            # Train the generator\n            with tf.GradientTape() as tape:\n                fake_data = self.gen(tf.random.normal((batch_size, self.noise_size)))\n                fake_output = self.disc(fake_data)\n                loss = self.generator_loss(fake_output)\n            gradients = tape.gradient(loss, self.gen.trainable_variables)\n            generator_optimizer.apply_gradients(zip(gradients, self.gen.trainable_variables))\n\n        # if epoch % 100 == 0:\n        disc_losses.append(np.mean(self.discriminator_loss(real_output, fake_output)))\n        gen_losses.append(np.mean(self.generator_loss(fake_output)))\n        print(f\"Epoch {epoch}: Discriminator Loss = {np.mean(self.discriminator_loss(real_output, fake_output))}, Generator Loss = {np.mean(self.generator_loss(fake_output))}\")\n    return disc_losses,gen_losses\n# Define the generator and discriminator networks\n  def generator(self):\n      model = tf.keras.Sequential([\n          tf.keras.layers.Dense(128, activation='relu', input_shape=(self.noise_size,)),\n          tf.keras.layers.Dense(256, activation='relu'),\n          tf.keras.layers.Dense(512, activation='relu'),\n#           tf.keras.layers.Dense(1024, activation='relu'),\n#            tf.keras.layers.Dense(2024, activation='relu'),\n#            tf.keras.layers.Dense(4048, activation='relu'),\n#            tf.keras.layers.Dense(2024, activation='relu'),\n           tf.keras.layers.Dense(1024, activation='relu'),\n          tf.keras.layers.Dense(512, activation='relu'),\n          tf.keras.layers.Dense(256, activation='relu'),\n          tf.keras.layers.Dense(128, activation='relu'),\n          tf.keras.layers.Dense(self.num_features,activation='softmax')\n      ])\n      # fake_data = model(input_noise)\n      return model\n\n  def discriminator(self):\n      model = tf.keras.Sequential([\n          tf.keras.layers.Dense(64, activation='relu', input_shape=(self.num_features,)),\n          tf.keras.layers.Dense(256, activation='relu'),\n          tf.keras.layers.Dense(512, activation='relu'),\n          # tf.keras.layers.Dense(1024, activation='relu'),\n          # tf.keras.layers.Dense(512, activation='relu'),\n          tf.keras.layers.Dense(256, activation='relu'),\n          # tf.keras.layers.Dense(128, activation='relu'),\n          tf.keras.layers.Dense(1, activation='sigmoid')\n      ])\n      # output = model(input_data)\n      return model\n\n# Define the loss function for the discriminator\n  def discriminator_loss(self,real_output, fake_output):\n      real_loss = tf.losses.binary_crossentropy(tf.ones_like(real_output), real_output)\n      fake_loss = tf.losses.binary_crossentropy(tf.zeros_like(fake_output), fake_output)\n      return real_loss + fake_loss\n\n# Define the loss function for the generator\n  def generator_loss(self,fake_output):\n      return tf.losses.binary_crossentropy(tf.ones_like(fake_output), fake_output)\n  def Predict(self,num_samples):\n    output = self.gen(tf.random.normal((num_samples, self.noise_size)))\n    final = np.zeros((num_samples,self.num_features))\n    for i in range(self.num_features):\n      final[:,i] = output[:,i]/th_out[i]\n    final = np.round(final)\n    return final\n  def plot_dist(self, num_samples, feature_names, save_path=None):\n    final = self.Predict(num_samples)\n    \n    # Calculate the number of rows and columns for the subplots\n    num_rows = 2\n    num_cols = 9\n    \n    # Create subplots\n    fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(30, 8))\n    \n    # Flatten the axes array for easier indexing\n    axes = axes.flatten()\n    \n    for i in range(min(self.num_features, num_rows * num_cols)):\n        unique_values, value_counts = np.unique(final[:, i], return_counts=True)\n        axes[i].bar(unique_values, value_counts)\n        axes[i].set_title(f'{feature_names[i]}')  # Use feature names as titles for each subplot\n\n    # Adjust layout to prevent clipping of titles\n    plt.tight_layout()\n\n    if save_path:\n        plt.savefig(save_path)\n\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-15T15:20:19.146736Z","iopub.execute_input":"2023-11-15T15:20:19.147289Z","iopub.status.idle":"2023-11-15T15:20:19.173397Z","shell.execute_reply.started":"2023-11-15T15:20:19.147255Z","shell.execute_reply":"2023-11-15T15:20:19.172471Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in raneg(len(encoded)):\n    G2 = GANS_Soft(encoded[i],100,20,2048)\n    disc,gen = G2.Fit()\n    G2.gen.save(f'/kaggle/working/model_sofmax{}')\n    G2.plot_dist(306419,'')","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fin = G1.Predict(306419)\nG1.plot_dist(306419)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]}]}